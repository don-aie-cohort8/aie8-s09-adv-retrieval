{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval Evaluations - generating RAGAS Golden Testsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library\n",
        "import getpass\n",
        "import os\n",
        "# import openai\n",
        "\n",
        "# Third-party packages\n",
        "# LangChain\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "# RAGAS\n",
        "# from ragas.embeddings import OpenAIEmbeddings # not available in 0.2.10\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.testset import TestsetGenerator\n",
        "from ragas.testset.graph import KnowledgeGraph, Node, NodeType\n",
        "from ragas.testset.persona import Persona\n",
        "from ragas.testset.synthesizers.single_hop.specific import (\n",
        "    SingleHopSpecificQuerySynthesizer,\n",
        ")\n",
        "from ragas.testset.transforms import (\n",
        "    HeadlinesExtractor,\n",
        "    HeadlineSplitter,\n",
        "    KeyphrasesExtractor,\n",
        "    apply_transforms,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\"))\n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings(model=\"text-embedding-3-small\"))\n",
        "# openai_client = openai.OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the source documents\n",
        "\n",
        "- use the LangChain CSVLoader to load the source documents\n",
        "- specify the metadata columns that will go into the metadata field\n",
        "- any other columns will go into the page_content field"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### our initial custom approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the effect of this is that we end up with an empty page_content property\n",
        "loader = CSVLoader(\n",
        "    file_path=f\"../data/Projects_with_Domains.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Project Title\",\n",
        "      \"Project Domain\",\n",
        "      \"Secondary Domain\",\n",
        "      \"Description\",\n",
        "      \"Judge Comments\",\n",
        "      \"Score\",\n",
        "      \"Project Name\",\n",
        "      \"Judge Score\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "ragas_usecase_data_attempt_1 = loader.load()\n",
        "\n",
        "# original code focused on populating the page_content only with the description\n",
        "# below I'm adding additional columns to improve RAGAS golden testset generation\n",
        "# QUESTION 1:  if the metadata content duplicates the page_content, is it really metadata?\n",
        "# QUESTION 2:  what does the structure of the original dataset tell us about the nature of it?  (is it structured or unstructured?)\n",
        "\n",
        "for doc in ragas_usecase_data_attempt_1:\n",
        "    title = doc.metadata.get(\"Project Title\", \"\")\n",
        "    domain = doc.metadata.get(\"Project Domain\", \"\")\n",
        "    secondary = doc.metadata.get(\"Secondary Domain\", \"\")\n",
        "    desc = doc.metadata.get(\"Description\", \"\")\n",
        "\n",
        "    doc.page_content = f\"{title}\\nDomain: {domain}\\nSecondary Domain: {secondary}\\nDescription: {desc}\".strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# number of documents\n",
        "len(ragas_usecase_data_attempt_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': '../data/Projects_with_Domains.csv', 'row': 0, 'Project Title': 'InsightAI 1', 'Project Domain': 'Security', 'Secondary Domain': 'Finance / FinTech', 'Description': 'A low-latency inference system for multimodal agents in autonomous systems.', 'Judge Comments': 'Technically ambitious and well-executed.', 'Score': '85', 'Project Name': 'Project Aurora', 'Judge Score': '9.5'}, page_content='InsightAI 1\\nDomain: Security\\nSecondary Domain: Finance / FinTech\\nDescription: A low-latency inference system for multimodal agents in autonomous systems.')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# content of the first document\n",
        "ragas_usecase_data_attempt_1[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "153"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# length of the first document's page_content\n",
        "len(ragas_usecase_data_attempt_1[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### revert to more of a standard approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# standard approach of using LangChain loaders\n",
        "# everything that is not specified in metadata_columns goes into the page_content context\n",
        "\n",
        "ragas_loader = CSVLoader(\n",
        "    file_path=\"../data/Projects_with_Domains.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Judge Comments\",\n",
        "      \"Score\",\n",
        "      \"Project Name\",\n",
        "      \"Judge Score\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "ragas_usecase_data = ragas_loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# number of documents\n",
        "len(ragas_usecase_data_attempt_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': '../data/Projects_with_Domains.csv', 'row': 0, 'Judge Comments': 'Technically ambitious and well-executed.', 'Score': '85', 'Project Name': 'Project Aurora', 'Judge Score': '9.5'}, page_content='Project Title: InsightAI 1\\nProject Domain: Security\\nSecondary Domain: Finance / FinTech\\nDescription: A low-latency inference system for multimodal agents in autonomous systems.')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# content of the first document\n",
        "ragas_usecase_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "176"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# length of the first document's page_content\n",
        "len(ragas_usecase_data[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAGAS Golden Testset Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the Vanilla Approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Documents appears to be too short (ie 100 tokens or less). Please provide longer documents.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m golden_dataset_attempt_1 = \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_with_langchain_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mragas_usecase_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestset_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/don-aie-cohort8/aie8-s09-adv-retrieval/.venv/lib/python3.13/site-packages/ragas/testset/synthesizers/generate.py:164\u001b[39m, in \u001b[36mTestsetGenerator.generate_with_langchain_docs\u001b[39m\u001b[34m(self, documents, testset_size, transforms, transforms_llm, transforms_embedding_model, query_distribution, run_config, callbacks, with_debugging_logs, raise_exceptions)\u001b[39m\n\u001b[32m    159\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    160\u001b[39m \u001b[38;5;250m        \u001b[39m\u001b[33;03m\"\"\"An embedding client was not provided. Provide an embedding through the transforms_embedding_model parameter. Alternatively you can provide your own transforms through the `transforms` parameter.\"\"\"\u001b[39;00m\n\u001b[32m    161\u001b[39m     )\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m transforms:\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m     transforms = \u001b[43mdefault_transforms\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransforms_llm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m        \u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransforms_embedding_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membedding_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;66;03m# convert the documents to Ragas nodes\u001b[39;00m\n\u001b[32m    171\u001b[39m nodes = []\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/don-aie-cohort8/aie8-s09-adv-retrieval/.venv/lib/python3.13/site-packages/ragas/testset/transforms/default.py:160\u001b[39m, in \u001b[36mdefault_transforms\u001b[39m\u001b[34m(documents, llm, embedding_model)\u001b[39m\n\u001b[32m    153\u001b[39m     transforms = [\n\u001b[32m    154\u001b[39m         summary_extractor,\n\u001b[32m    155\u001b[39m         node_filter,\n\u001b[32m    156\u001b[39m         Parallel(summary_emb_extractor, theme_extractor, ner_extractor),\n\u001b[32m    157\u001b[39m         ner_overlap_sim,\n\u001b[32m    158\u001b[39m     ]\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    161\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDocuments appears to be too short (ie 100 tokens or less). Please provide longer documents.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    162\u001b[39m     )\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m transforms\n",
            "\u001b[31mValueError\u001b[39m: Documents appears to be too short (ie 100 tokens or less). Please provide longer documents."
          ]
        }
      ],
      "source": [
        "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n",
        "golden_dataset_attempt_1 = generator.generate_with_langchain_docs(ragas_usecase_data, testset_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Doc 0: 39 tokens\n",
            "Doc 1: 38 tokens\n",
            "Doc 2: 37 tokens\n",
            "Doc 3: 40 tokens\n",
            "Doc 4: 40 tokens\n",
            "Doc 5: 38 tokens\n",
            "Doc 6: 41 tokens\n",
            "Doc 7: 38 tokens\n",
            "Doc 8: 38 tokens\n",
            "Doc 9: 45 tokens\n",
            "Doc 10: 44 tokens\n",
            "Doc 11: 39 tokens\n",
            "Doc 12: 41 tokens\n",
            "Doc 13: 36 tokens\n",
            "Doc 14: 44 tokens\n",
            "Doc 15: 42 tokens\n",
            "Doc 16: 35 tokens\n",
            "Doc 17: 41 tokens\n",
            "Doc 18: 37 tokens\n",
            "Doc 19: 39 tokens\n",
            "Doc 20: 41 tokens\n",
            "Doc 21: 40 tokens\n",
            "Doc 22: 37 tokens\n",
            "Doc 23: 33 tokens\n",
            "Doc 24: 39 tokens\n",
            "Doc 25: 37 tokens\n",
            "Doc 26: 38 tokens\n",
            "Doc 27: 36 tokens\n",
            "Doc 28: 37 tokens\n",
            "Doc 29: 41 tokens\n",
            "Doc 30: 42 tokens\n",
            "Doc 31: 36 tokens\n",
            "Doc 32: 41 tokens\n",
            "Doc 33: 41 tokens\n",
            "Doc 34: 38 tokens\n",
            "Doc 35: 34 tokens\n",
            "Doc 36: 41 tokens\n",
            "Doc 37: 44 tokens\n",
            "Doc 38: 39 tokens\n",
            "Doc 39: 40 tokens\n",
            "Doc 40: 41 tokens\n",
            "Doc 41: 39 tokens\n",
            "Doc 42: 39 tokens\n",
            "Doc 43: 40 tokens\n",
            "Doc 44: 43 tokens\n",
            "Doc 45: 40 tokens\n",
            "Doc 46: 37 tokens\n",
            "Doc 47: 42 tokens\n",
            "Doc 48: 40 tokens\n",
            "Doc 49: 39 tokens\n"
          ]
        }
      ],
      "source": [
        "# calculate the length in tokens of the source documents\n",
        "\n",
        "token_splitter = TokenTextSplitter()\n",
        "\n",
        "for i, doc in enumerate(ragas_usecase_data):\n",
        "    tokens = token_splitter._tokenizer.encode(doc.page_content)\n",
        "    print(f\"Doc {i}: {len(tokens)} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### using RAGAS Knowledge Graph functionality\n",
        "\n",
        "- unroll the golden testset process to have more control over generation\n",
        "- custom personas are not necessary but showcase useful RAGAS functionality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### create the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "kg = KnowledgeGraph()\n",
        "\n",
        "for doc in ragas_usecase_data:\n",
        "    kg.nodes.append(\n",
        "        Node(\n",
        "            type=NodeType.DOCUMENT,\n",
        "            properties={\"page_content\": doc.page_content, \"document_metadata\": doc.metadata}\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KnowledgeGraph(nodes: 50, relationships: 0)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the initial graph\n",
        "\n",
        "kg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6854d1e16d9470faf68e6ce8fa2abc4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlinesExtractor:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "895bd3b2c0d94a7182a1f5c8f0447d72",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlineSplitter:   0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56524aa7053a4cadac2f9be486d9621a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying KeyphrasesExtractor:   0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Property 'keyphrases' already exists in node 'c2c497'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'e76b3c'. Skipping!\n",
            "Property 'keyphrases' already exists in node '677e26'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'da968f'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'd4b43d'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'ef0b58'. Skipping!\n",
            "Property 'keyphrases' already exists in node '8d31aa'. Skipping!\n",
            "Property 'keyphrases' already exists in node '8c2091'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'e6aea8'. Skipping!\n",
            "Property 'keyphrases' already exists in node '8da5aa'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'f61639'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'df7dc4'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'bde00c'. Skipping!\n",
            "Property 'keyphrases' already exists in node '609339'. Skipping!\n",
            "Property 'keyphrases' already exists in node '9bfc1a'. Skipping!\n",
            "Property 'keyphrases' already exists in node '117c60'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'ec56c3'. Skipping!\n",
            "Property 'keyphrases' already exists in node '0b4de3'. Skipping!\n",
            "Property 'keyphrases' already exists in node '50d59b'. Skipping!\n",
            "Property 'keyphrases' already exists in node '7c9f8e'. Skipping!\n",
            "Property 'keyphrases' already exists in node '488ef7'. Skipping!\n",
            "Property 'keyphrases' already exists in node '762ede'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'eaaa55'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'd061a5'. Skipping!\n",
            "Property 'keyphrases' already exists in node '559737'. Skipping!\n",
            "Property 'keyphrases' already exists in node '93a3b7'. Skipping!\n",
            "Property 'keyphrases' already exists in node '93d8df'. Skipping!\n",
            "Property 'keyphrases' already exists in node '9891f6'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'a74b5d'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'd9cab0'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'd8e1d7'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'e03a3b'. Skipping!\n",
            "Property 'keyphrases' already exists in node '03f84f'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'f74fa0'. Skipping!\n",
            "Property 'keyphrases' already exists in node '7e2efa'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'e8fb06'. Skipping!\n",
            "Property 'keyphrases' already exists in node '78d9f8'. Skipping!\n",
            "Property 'keyphrases' already exists in node '4cac70'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'e4e20a'. Skipping!\n",
            "Property 'keyphrases' already exists in node '436f95'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'ae3dfb'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'e14771'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'd123bb'. Skipping!\n",
            "Property 'keyphrases' already exists in node '5cc3f0'. Skipping!\n",
            "Property 'keyphrases' already exists in node '65751c'. Skipping!\n",
            "Property 'keyphrases' already exists in node '122e01'. Skipping!\n",
            "Property 'keyphrases' already exists in node '15050d'. Skipping!\n",
            "Property 'keyphrases' already exists in node 'b08965'. Skipping!\n",
            "Property 'keyphrases' already exists in node '93bbe6'. Skipping!\n",
            "Property 'keyphrases' already exists in node '6f5f8c'. Skipping!\n"
          ]
        }
      ],
      "source": [
        "headline_extractor = HeadlinesExtractor(llm=generator_llm)\n",
        "headline_splitter = HeadlineSplitter(max_tokens=1500)\n",
        "keyphrase_extractor = KeyphrasesExtractor(llm=generator_llm)\n",
        "\n",
        "transforms = [\n",
        "    headline_extractor,\n",
        "    headline_splitter,\n",
        "    keyphrase_extractor\n",
        "]\n",
        "\n",
        "apply_transforms(kg, transforms=transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KnowledgeGraph(nodes: 100, relationships: 0)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# check the graph after applying the transforms\n",
        "\n",
        "kg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KnowledgeGraph(nodes: 100, relationships: 0)"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kg.save(\"usecase_data_kg.json\")\n",
        "usecase_data_kg = KnowledgeGraph.load(\"usecase_data_kg.json\")\n",
        "usecase_data_kg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### identify personas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Persona               | Use Case Type                             | Derived From                                                                |\n",
        "| --------------------- | ----------------------------------------- | --------------------------------------------------------------------------- |\n",
        "| Decision Analyst      | **Asking / Seeking Information**          | “Decision support and information interpretation dominate work-related use” |\n",
        "| Domain Researcher     | **Knowledge Graph & Multi-hop Retrieval** | Multi-domain structure in `Projects_with_Domains.csv`                       |\n",
        "| Instructional Creator | **Practical Guidance / Tutoring**         | Education & self-learning patterns (10% of usage)                           |\n",
        "| AI Practitioner       | **Evaluation & Coding Assistance**        | Work-related “Doing” messages (40% overall)                                 |\n",
        "| Creative Strategist   | **Self-Expression / Ideation**            | Growth of “Expressing” and “Creative Guidance” segments                     |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "persona_decision_analyst = Persona(\n",
        "    name=\"Decision Analyst\",\n",
        "    role_description=(\n",
        "        \"Uses AI for analytical reasoning and decision support. \"\n",
        "        \"Seeks data-driven insights, summaries, and structured outputs to inform business or policy decisions. \"\n",
        "        \"Values concise factual responses, traceable evidence, and cost-effective solutions.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "persona_domain_researcher = Persona(\n",
        "    name=\"Domain Researcher\",\n",
        "    role_description=(\n",
        "        \"Explores multi-domain knowledge sources (e.g., education, health, finance, engineering). \"\n",
        "        \"Prefers context-rich retrieval with citations and nuanced synthesis. \"\n",
        "        \"Often asks cross-domain 'why/how' questions requiring reasoning beyond surface-level facts.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "persona_instructional_creator = Persona(\n",
        "    name=\"Instructional Creator\",\n",
        "    role_description=(\n",
        "        \"Designs educational or training materials using AI. \"\n",
        "        \"Relies on clear, pedagogical explanations and consistent tone. \"\n",
        "        \"Frequently asks for examples, analogies, or simplified explanations for learners.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "persona_ai_practitioner = Persona(\n",
        "    name=\"AI Practitioner\",\n",
        "    role_description=(\n",
        "        \"Implements and evaluates retrieval-augmented systems. \"\n",
        "        \"Needs structured, reproducible outputs like JSON schemas, test cases, and evaluation metrics. \"\n",
        "        \"Focuses on precision, recall, and factual grounding when comparing retrievers or datasets.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "persona_creative_strategist = Persona(\n",
        "    name=\"Creative Strategist\",\n",
        "    role_description=(\n",
        "        \"Uses AI for ideation, storytelling, and persuasive communication. \"\n",
        "        \"Seeks novel phrasing, emotional resonance, and creative reframing of ideas. \"\n",
        "        \"Frequently explores role-play or scenario-based reasoning.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "personas = [\n",
        "    persona_decision_analyst,\n",
        "    persona_domain_researcher,\n",
        "    persona_instructional_creator,\n",
        "    persona_ai_practitioner,\n",
        "    persona_creative_strategist,\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### define query behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "query_distibution = [\n",
        "    (\n",
        "        SingleHopSpecificQuerySynthesizer(llm=generator_llm, property_name=\"headlines\"),\n",
        "        0.5,\n",
        "    ),\n",
        "    (\n",
        "        SingleHopSpecificQuerySynthesizer(\n",
        "            llm=generator_llm, property_name=\"keyphrases\"\n",
        "        ),\n",
        "        0.5,\n",
        "    ),\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### generate testset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "generator = TestsetGenerator(\n",
        "    llm=generator_llm,\n",
        "    embedding_model=generator_embeddings,\n",
        "    knowledge_graph=usecase_data_kg,\n",
        "    persona_list=personas,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4294fd6d3a44822852a2ed0aecc59ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0be411155512402eb1d87d7b2ca6ba0e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>How Security important in InsightAI project wi...</td>\n",
              "      <td>[Project Title: InsightAI 1\\nProject Domain: S...</td>\n",
              "      <td>The InsightAI project is focused on the domain...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Could you provide a detailed explanation of ho...</td>\n",
              "      <td>[Project Title: ShopSmart 2\\nProject Domain: D...</td>\n",
              "      <td>The ShopSmart 2 project falls under the second...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what WealthifyAI 3 do in medical imaging?</td>\n",
              "      <td>[Project Title: WealthifyAI 3\\nProject Domain:...</td>\n",
              "      <td>WealthifyAI 3 is a medical imaging solution th...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What is MediMind 4 and how does it integrate a...</td>\n",
              "      <td>[Project Title: MediMind 4\\nProject Domain: E‑...</td>\n",
              "      <td>MediMind 4 is a project situated primarily in ...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Could you explain the primary domain focus of ...</td>\n",
              "      <td>[Project Title: AutoMate 5\\nProject Domain: Fi...</td>\n",
              "      <td>The AutoMate 5 project primarily focuses on th...</td>\n",
              "      <td>single_hop_specifc_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  How Security important in InsightAI project wi...   \n",
              "1  Could you provide a detailed explanation of ho...   \n",
              "2          what WealthifyAI 3 do in medical imaging?   \n",
              "3  What is MediMind 4 and how does it integrate a...   \n",
              "4  Could you explain the primary domain focus of ...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [Project Title: InsightAI 1\\nProject Domain: S...   \n",
              "1  [Project Title: ShopSmart 2\\nProject Domain: D...   \n",
              "2  [Project Title: WealthifyAI 3\\nProject Domain:...   \n",
              "3  [Project Title: MediMind 4\\nProject Domain: E‑...   \n",
              "4  [Project Title: AutoMate 5\\nProject Domain: Fi...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  The InsightAI project is focused on the domain...   \n",
              "1  The ShopSmart 2 project falls under the second...   \n",
              "2  WealthifyAI 3 is a medical imaging solution th...   \n",
              "3  MediMind 4 is a project situated primarily in ...   \n",
              "4  The AutoMate 5 project primarily focuses on th...   \n",
              "\n",
              "                       synthesizer_name  \n",
              "0  single_hop_specifc_query_synthesizer  \n",
              "1  single_hop_specifc_query_synthesizer  \n",
              "2  single_hop_specifc_query_synthesizer  \n",
              "3  single_hop_specifc_query_synthesizer  \n",
              "4  single_hop_specifc_query_synthesizer  "
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "golden_testset = generator.generate(testset_size=10, query_distribution=query_distibution)\n",
        "golden_testset.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### push to hugging face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate the json version of the testset\n",
        "golden_testset_jsonl = golden_testset.to_jsonl(\"golden_testset.jsonl\")\n",
        "\n",
        "# generate the hugging face version of the testset\n",
        "golden_testset_hf = golden_testset.to_hf_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PLEASE NOTE...\n",
        "\n",
        "- in the end the golden testset generation process still creates a graph with no edges\n",
        "- the concept of custom personas is still valuable and helps to design better systems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
          ]
        },
        {
          "ename": "HfHubHTTPError",
          "evalue": "(Request ID: Root=1-68ec3190-705bdd3276cfa9d86d87183b;7d52f841-940a-46af-9e6f-41daac25da17)\n\n403 Forbidden: You don't have the rights to create a dataset under the namespace \"your-username\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/don-aie-cohort8/aie8-s09-adv-retrieval/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:407\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/don-aie-cohort8/aie8-s09-adv-retrieval/.venv/lib/python3.13/site-packages/requests/models.py:1026\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
            "\u001b[31mHTTPError\u001b[39m: 403 Client Error: Forbidden for url: https://huggingface.co/api/repos/create",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# 4) Ensure dataset repo exists\u001b[39;00m\n\u001b[32m     22\u001b[39m api = HfApi()\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mREPO_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPRIVATE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 5) Light schema guard (great for a class to show “fail fast”)\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_assert_cols\u001b[39m(ds):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/don-aie-cohort8/aie8-s09-adv-retrieval/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/don-aie-cohort8/aie8-s09-adv-retrieval/.venv/lib/python3.13/site-packages/huggingface_hub/hf_api.py:3779\u001b[39m, in \u001b[36mHfApi.create_repo\u001b[39m\u001b[34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[39m\n\u001b[32m   3777\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m RepoUrl(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.endpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   3778\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m HfHubHTTPError:\n\u001b[32m-> \u001b[39m\u001b[32m3779\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m   3780\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3781\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/don-aie-cohort8/aie8-s09-adv-retrieval/.venv/lib/python3.13/site-packages/huggingface_hub/hf_api.py:3766\u001b[39m, in \u001b[36mHfApi.create_repo\u001b[39m\u001b[34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[39m\n\u001b[32m   3763\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   3765\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3766\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3767\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   3768\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err.response.status_code == \u001b[32m409\u001b[39m:\n\u001b[32m   3769\u001b[39m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/don-aie-cohort8/aie8-s09-adv-retrieval/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:471\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m403\u001b[39m:\n\u001b[32m    466\u001b[39m     message = (\n\u001b[32m    467\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    468\u001b[39m         + \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    469\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMake sure your token has the correct permissions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m416\u001b[39m:\n\u001b[32m    474\u001b[39m     range_header = response.request.headers.get(\u001b[33m\"\u001b[39m\u001b[33mRange\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mHfHubHTTPError\u001b[39m: (Request ID: Root=1-68ec3190-705bdd3276cfa9d86d87183b;7d52f841-940a-46af-9e6f-41daac25da17)\n\n403 Forbidden: You don't have the rights to create a dataset under the namespace \"your-username\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions."
          ]
        }
      ],
      "source": [
        "# --- Minimal push cell using your two calls ---\n",
        "\n",
        "import os\n",
        "from huggingface_hub import HfApi, login\n",
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "\n",
        "REPO_ID = \"dwb2023/projects-golden-testset\"   # e.g., \"donbr/projects-golden-testset\"\n",
        "PRIVATE = False\n",
        "REQUIRED_COLUMNS = {\"question\", \"answer\", \"reference_context\"}  # adjust to your schema\n",
        "\n",
        "# 1) (Optional) Export JSONL for local inspection\n",
        "golden_testset.to_jsonl(\"golden_testset.jsonl\")  # writes ./golden_testset.jsonl\n",
        "\n",
        "# 2) Convert to HF dataset (Dataset or DatasetDict)\n",
        "hf_ds = golden_testset.to_hf_dataset()\n",
        "\n",
        "# 3) Login (no hardcoded tokens)\n",
        "token = os.environ[\"HF_TOKEN\"]\n",
        "login(token=token)\n",
        "\n",
        "# 4) Ensure dataset repo exists\n",
        "api = HfApi()\n",
        "api.create_repo(repo_id=REPO_ID, repo_type=\"dataset\", private=PRIVATE, exist_ok=True)\n",
        "\n",
        "# 5) Light schema guard (great for a class to show “fail fast”)\n",
        "def _assert_cols(ds):\n",
        "    missing = REQUIRED_COLUMNS - set(ds.column_names)\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing required columns: {missing}. Present: {ds.column_names}\")\n",
        "\n",
        "if isinstance(hf_ds, DatasetDict):\n",
        "    for split, ds in hf_ds.items(): _assert_cols(ds)\n",
        "else:\n",
        "    _assert_cols(hf_ds)\n",
        "\n",
        "# 6) Push (handles Dataset or DatasetDict)\n",
        "hf_ds.push_to_hub(REPO_ID, private=PRIVATE)\n",
        "\n",
        "# 7) Verify\n",
        "loaded = load_dataset(REPO_ID)\n",
        "print(loaded)\n",
        "print(\"✅ Pushed and verified.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
