# **Session 09: SDG & LangGraph ‚Äì Code Walkthrough Summary**

**Date:** 2025-10-13    **Presenter:** Don B
**Repo:** [`aie8-s09-adv-retrieval`](https://github.com/don-aie-cohort8/aie8-s09-adv-retrieval)

**Key Artifacts:**

- Notebooks
  - [RAGAS version and generation issues](https://github.com/don-aie-cohort8/aie8-s09-adv-retrieval/blob/main/notebooks/session09-adv-retrieval-ragas.ipynb)
  - [Refactoring the langchain and retrievers](https://github.com/don-aie-cohort8/aie8-s09-adv-retrieval/blob/main/notebooks/session09-adv-retrieval.ipynb)

---

## üß† Overview

This session reviewed major lessons from **Sessions 7-9** and walked through code refactors improving the **RAGAS + LangGraph** workflow.
Focus areas included dataset reliability, retriever design, version migration (0.2 ‚Üí 0.3), and observability.

---

## üîÅ RAGAS Version Migration

| Area             | v0.2 Behavior           | v0.3 Change                                                      |
| ---------------- | ----------------------- | ---------------------------------------------------------------- |
| Embeddings       | Uses LangChain wrappers | Direct OpenAI/Other API calls (expected deprecation of wrappers) |
| Graph Generation | Stable for single-hop   | Multi-hop still unstable ‚Äî start simple                          |
| Persona Handling | Limited and text-based  | More robust generation via structured prompts                    |

**Key Tip:** Begin with **single-hop** data generation before multi-hop to avoid silent failures.

---

## üß© Dataset and Tokenization Findings

* The *‚Äúprojects with domains‚Äù* dataset is **semi-structured** and too small ‚Üí frequent
  `_Documents appear to be too short (< 100 tokens)_` errors.
* **Duplication & mismatched labels** (‚Äúsales CRM‚Äù tagged as healthcare) ‚Üí unreliable semantic retrieval.
* **Fix:** Concatenate `domain` + `secondary_domain` + `description` into `page_content`.
* Validate length via `TokenTextSplitter`; target ‚â• 100 tokens per doc.

---

## üß± LangChain Document Loader Behavior

* All declared metadata fields ‚Üí `metadata` dict.
* All other fields ‚Üí `page_content`.
* Earlier notebooks stored everything in metadata (‚Äúkitchen sink‚Äù).
  ‚úÖ **Refactor:** Keep meaningful text in `page_content` for retrieval fidelity.

---

## üßç‚Äç‚ôÇÔ∏è Persona-Driven Test Set Generation

* Implemented **custom personas** (e.g., ChatGPT user archetypes) to drive query diversity.
* Compared v0.2 vs v0.3 behavior: 0.3 produces richer synthetic data and more queries per persona.
* Personas plugged directly into the generator ‚Üí observable impact on query distributions.

---

## üßÆ Retriever & Vector Store Architecture

* **Vector Stores:** Semantic (Qdrant), Parent/Child, and in-memory.
* **Retrievers:** Compression, Multi-Query, and Ensemble.
* All retrievers grouped under a unified API interface ‚Üí ready for **FastAPI/FastMCP** exposure.
* Mirrors real-world RAG pattern: parent context chunks for coarse recall + fine semantic retrieval.

---

## üìä Evaluation & Observability

* Helpers normalize outputs and run single/batch queries across retrievers.
* **Duplicates** stem from CSV data, not retriever logic.
* Validation via **LangSmith traces** and token-level inspection ensures pipeline truthfulness.
* Demonstrates how to extend these retrievers for RAGAS metric evaluation and LangSmith reporting.

---

## üåê Publishing & Provenance

* Pushed source and generated datasets to **Hugging Face (`dwb2023`)** for transparency and reproducibility.
* Each dataset includes metadata descriptions and link back to the original class notebook
  ‚Üí see [`session09-adv-retrieval-ragas.ipynb`](https://github.com/don-aie-cohort8/aie8-s09-adv-retrieval/blob/main/notebooks/session09-adv-retrieval-ragas.ipynb)

---

## ‚úÖ Key Takeaways

1. Start simple (single-hop) ‚Üí multi-hop later.
2. Preserve semantic content in `page_content`; avoid metadata overload.
3. Maintain ‚â• 100 tokens per doc to prevent RAGAS rejections.
4. Use personas to simulate authentic user queries.
5. Expose retrievers via MCP / FastAPI for LLM integration.
6. Track runs and validate via LangSmith & Hugging Face datasets.
