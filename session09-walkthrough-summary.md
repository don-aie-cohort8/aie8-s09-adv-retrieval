# **Session 09: SDG & LangGraph â€“ Code Walkthrough Summary**

**Date:** 2025-10-13    **Presenter:** Don B
**Repo:** [`aie8-s09-adv-retrieval`](https://github.com/don-aie-cohort8/aie8-s09-adv-retrieval)

---

## ğŸ§  Overview

This session reviewed major lessons from **Sessions 7-9** and walked through code refactors improving the **RAGAS + LangGraph** workflow.
Focus areas included dataset reliability, retriever design, version migration (0.2 â†’ 0.3), and observability.

---

## ğŸ” RAGAS Version Migration

| Area             | v0.2 Behavior           | v0.3 Change                                                      |
| ---------------- | ----------------------- | ---------------------------------------------------------------- |
| Embeddings       | Uses LangChain wrappers | Direct OpenAI/Other API calls (expected deprecation of wrappers) |
| Graph Generation | Stable for single-hop   | Multi-hop still unstable â€” start simple                          |
| Persona Handling | Limited and text-based  | More robust generation via structured prompts                    |

**Key Tip:** Begin with **single-hop** data generation before multi-hop to avoid silent failures.

---

## ğŸ§© Dataset and Tokenization Findings

* The *â€œprojects with domainsâ€* dataset is **semi-structured** and too small â†’ frequent
  `_Documents appear to be too short (< 100 tokens)_` errors.
* **Duplication & mismatched labels** (â€œsales CRMâ€ tagged as healthcare) â†’ unreliable semantic retrieval.
* **Fix:** Concatenate `domain` + `secondary_domain` + `description` into `page_content`.
* Validate length via `TokenTextSplitter`; target â‰¥ 100 tokens per doc.

---

## ğŸ§± LangChain Document Loader Behavior

* All declared metadata fields â†’ `metadata` dict.
* All other fields â†’ `page_content`.
* Earlier notebooks stored everything in metadata (â€œkitchen sinkâ€).
  âœ… **Refactor:** Keep meaningful text in `page_content` for retrieval fidelity.

---

## ğŸ§â€â™‚ï¸ Persona-Driven Test Set Generation

* Implemented **custom personas** (e.g., ChatGPT user archetypes) to drive query diversity.
* Compared v0.2 vs v0.3 behavior: 0.3 produces richer synthetic data and more queries per persona.
* Personas plugged directly into the generator â†’ observable impact on query distributions.

---

## ğŸ§® Retriever & Vector Store Architecture

* **Vector Stores:** Semantic (Qdrant), Parent/Child, and in-memory.
* **Retrievers:** Compression, Multi-Query, and Ensemble.
* All retrievers grouped under a unified API interface â†’ ready for **FastAPI/FastMCP** exposure.
* Mirrors real-world RAG pattern: parent context chunks for coarse recall + fine semantic retrieval.

---

## ğŸ“Š Evaluation & Observability

* Helpers normalize outputs and run single/batch queries across retrievers.
* **Duplicates** stem from CSV data, not retriever logic.
* Validation via **LangSmith traces** and token-level inspection ensures pipeline truthfulness.
* Demonstrates how to extend these retrievers for RAGAS metric evaluation and LangSmith reporting.

---

## ğŸŒ Publishing & Provenance

* Pushed source and generated datasets to **Hugging Face (`dwb2023`)** for transparency and reproducibility.
* Each dataset includes metadata descriptions and link back to the original class notebook
  â†’ see [`session09-adv-retrieval-ragas.ipynb`](https://github.com/don-aie-cohort8/aie8-s09-adv-retrieval/blob/main/notebooks/session09-adv-retrieval-ragas.ipynb)

---

## âœ… Key Takeaways

1. Start simple (single-hop) â†’ multi-hop later.
2. Preserve semantic content in `page_content`; avoid metadata overload.
3. Maintain â‰¥ 100 tokens per doc to prevent RAGAS rejections.
4. Use personas to simulate authentic user queries.
5. Expose retrievers via MCP / FastAPI for LLM integration.
6. Track runs and validate via LangSmith & Hugging Face datasets.
